import random, math, time
from urllib.parse import urlparse
import openai, os, validators, whois
from dotenv import load_dotenv

load_dotenv()
# Phishing Detector
# Idea s...ingenuity, prototypes
# SCALE: 1-5
# if less than pass: NOT RELIABLE AT ALLL 
# if barely pass: then less reliable 
# if pass well: then alr, reliable
AUTHENTIC_WEBSITES_LIST = [
    "google.com", "youtube.com", "facebook.com", "instagram.com", "chatgpt.com", "x.com",
      "whatsapp.com", "wikipedia.org", "reddit.com", "yahoo.co.jp", "yahoo.com", "yandex.ru",
      "tiktok.com", "amazon.com", "baidu.com", "bet.br", "microsoftonline.com", "linkedin.com",
      "netflix.com", "naver.com", "live.com", "dzen.ru", "office.com", "bing.com", "temu.com",
      "pinterest.com", "bilibili.com", "microsoft.com", "twitch.tv", "vk.com", "mail.ru", 
      "news.yahoo.co.jp", "sharepoint.com", "fandom.com", "globo.com", "canva.com", "weather.com",
      "samsung.com", "t.me", "duckduckgo.com", "nytimes.com", "zoom.us", "spotify.com", "discord.com",
      "apple.com", "imdb.com", "docker.com", "ebay.com", "aliexpress.com", "bbc.com",
      "stackoverflow.com", "quora.com", "github.com", "paypal.com", "adobe.com", "wordpress.org",
        "etsy.com", "cnn.com", "bbc.co.uk", "theguardian.com", "huffpost.com", "forbes.com",
        "buzzfeed.com", "usatoday.com", "reuters.com", "cnet.com", "techcrunch.com", "wired.com",
        "theverge.com", "engadget.com", "arstechnica.com", "gizmodo.com", "venturebeat.com",
        "mashable.com", "businessinsider.com", "bloomberg.com", "ft.com", "wsj.com", "nytimes.com",
        "theatlantic.com", "vox.com", "slate.com", "politico.com", "axios.com", "buzzfeednews.com",
        "thehill.com", "nationalgeographic.com", "history.com", "smithsonianmag.com", "scientificamerican.com",
        "nature.com", "sciencemag.org", "livescience.com", "space.com", "newscientist.com",
        "popularmechanics.com", "wired.co.uk", "gizmodo.co.uk", "techradar.com", "cnet.co.uk",
        "thetimes.co.uk", "telegraph.co.uk", "independent.co.uk", "mirror.co.uk", "dailymail.co.uk",
        "theguardian.co.uk", "bbc.co.uk", "sky.com", "itv.com", "channel4.com", "channel5.com",
        "metro.co.uk", "eveningstandard.co.uk", "thetimesofindia.com", "hindustantimes.com",
        "thehindu.com", "indianexpress.com", "timesnownews.com", "ndtv.com", "moneycontrol.com",
        "business-standard.com", "livemint.com", "firstpost.com", "news18.com", "zomato.com",
        "swiggy.com", "flipkart.com", "snapdeal.com", "paytm.com", "olx.in", "quikr.com",
        "bookmyshow.com", "irctc.co.in", "makeMyTrip.com", "cleartrip.com", "goibibo.com",
        "yatra.com", "redbus.in", "cleartrip.com", "indigoairlines.in", "spicejet.com",
        "airindia.in", "jetairways.com", "vistara.com", "goair.in", "indigo.in", "airasia.com",
        "makemytrip.com", "cleartrip.com", "yatra.com", "redbus.in", "goibibo.com",
        "bookmyshow.com", "zomato.com", "swiggy.com", "flipkart.com", "snapdeal.com",
        "paytm.com", "olx.in", "quikr.com", "bookmyshow.com", "irctc.co.in", "makeMyTrip.com",
        "cleartrip.com", "goibibo.com", "yatra.com", "redbus.in", "cleartrip.com",
        "indigoairlines.in", "spicejet.com", "airindia.in", "jetairways.com", "vistara.com",
        "goair.in", "indigo.in", "airasia.com", "makemytrip.com", "cleartrip.com", "yatra.com",
        "redbus.in", "goibibo.com", "bookmyshow.com", "zomato.com", "swiggy.com", "flipkart.com", "docs.python.org", "python.org"] 


sus_patterns = ["login", "secure", "update", "verify", "account", "confirm", "login", "signin", "signup", "register", "profile", "settings", "admin",
    "dashboard", "payment", "checkout", "cart", "order", "invoice", "receipt", "transaction", 
    "webmail", "validation", "auth", "authentication", "support", "security", "signin", "signup", "password", "banking", 
    "creditcard", "paypal", "stripe", "checkout", "cart", "order", "invoice", "receipt", "transaction",
    "alert", "warning", "error", "issue", "problem", "suspicious", "phishing", "scam", "fraud", "malware",
    "virus", "trojan", "ransomware", "spyware", "adware", "keylogger", "hacker", "hack", "breach", "compromise",
    "last-chance", "urgent", "immediate-action", "act-now", "limited-time", "exclusive-offer", "special-deal",
    "free-gift", "prize", "reward", "win", "lottery", "sweepstakes", "giveaway", "contest", "raffle"]


# The weights for each checker
# These weights are used to calculate the final score
CHECKER_WEIGHTS = {
    "prefix": 0.50,
    "tld": 0.50,
    "ip": 0.5,
    "length": 0.50,
    "random_char": 0.5,
    "suspicious": 0.75,
    "whois_age": 0.75
}


BlackList = []
# global variables
# Boolean Variables


UrlLink = input("Enter the URL to validate (start with http://): ")

print("Validating URL:", UrlLink)

# Helper Functions

def get_base_domain(netloc):
    domain = netloc.split(':')[0]
    parts = domain.split('.')
    if len(parts) > 2:
        domain = '.'.join(parts[-2:])
    return domain


def get_whois_info(url):
    try:
        domain = urlparse(url).netloc
        whois_info = whois.whois(domain)
        age = whois_info.creation_date
        if age:
            age = age[0] if isinstance(age, list) else age
            if isinstance(age, str):
                age = time.strptime(age, "%Y-%m-%d %H:%M:%S")
            elif isinstance(age, int):
                age = time.localtime(age)
            else:
                return 0.50
            current_time = time.localtime()
            age_in_days = (time.mktime(current_time) - time.mktime(age)) / (24 * 3600)
            if age_in_days < 30:
                return 0.25
            elif age_in_days < 365:
                return 0.50
            else:
                return 1.0
        else:
            return 0.50
    except Exception as e:
        print(f"Error fetching WHOIS info: {e}")
        return 0.50


# PhishingDetector Class
class PhishingDetector:
    def __init__(self, URL):
        self.url = URL
        self.parsed = urlparse(self.url)
        self.is_safe = True
        self.weights = CHECKER_WEIGHTS
        self.p_score = 1
        self.t_score = 1    
        self.d_score = 1
        self.w_score = 1
    # parses the URL and extracts components
    # such as scheme, domain, path, and query parameters
    def parse_url(self):
        return self.parsed

    # Scrutinizes the URL to determine if it is phishing
    # or not based on various checks 
    def identify_tests(self, p_score, t_score, d_score, w_score):
        # Identify the tests that will be run
        print("Running tests on the URL...")
        score_map = {p_score: "Prefix_score", t_score:"TLD_source", d_score:"Domain_source", w_score:"Whois_age_score"}
        for test_score in score_map.keys():
            if test_score > 0.75:
                print( f"Test case {score_map[test_score]} passed with {test_score} out of 1.0.")
            elif test_score >= 0.5 and test_score <= 0.75:
                print( f"Test case {score_map[test_score]} passed with {test_score} out of 1.0, but it is a bit sketchy.")
            elif test_score < 0.5:
                print(f"Test case {score_map[test_score]} failed with {test_score} out of 1.0, it is very sketchy.")

    def scrutinize(self):
        base_domain = get_base_domain(self.parsed.netloc)
        if base_domain in AUTHENTIC_WEBSITES_LIST or self.parsed.netloc in AUTHENTIC_WEBSITES_LIST:
            # Check if the URL is in the list of authentic websites
            self.is_safe = True
            final_score = 4.0
            log = "The URL is safe."
        elif base_domain in BlackList:
            # Check if the URL is in the blacklist
            self.is_safe = False
            final_score = 0.0
            log = "The URL is phishing and has already been blacklisted."
        else:
            prefix_score = self.prefix_checker(self.url)
            tld_score = self.TLD_checker()
            domain_score = self.domain_heuristics()
            whois_age_score = get_whois_info(self.url)
            
            # Calculate the final score based on the weights
            prefix_score = self.weights["prefix"] * (prefix_score)
            tld_score = self.weights["tld"] * (tld_score)
            whois_age_score = self.weights["whois_age"] * (whois_age_score)
            self.p_score = prefix_score
            self.t_score = tld_score
            self.d_score = domain_score
            self.w_score = whois_age_score
            
        
            # Calculate the final score
            final_score = (prefix_score + tld_score + domain_score + whois_age_score)
            # Determine if the URL is phishing based on the final score
            final_score = round(final_score, 2)
            if final_score < 2:
                self.is_safe = False
                log = "The URL is phishing."
                BlackList.append(self.parsed.netloc)
                log += (" The URL has been added to the blacklist.")
                # print(log)
            else:
                self.is_safe = True
                log = "The URL is safe."
                # print(log) 
        trust_review = self.trust_ranking(final_score)
        return trust_review, str(final_score) + " out of 4.0", log      
        
    def domain_heuristics(self):
        # Perform various checks on the URL to determine its authenticity
        ip_score = self.IP_address_checker()
        length_score = self.length_checker()
        random_char_score = self.random_char_checker()
        sus_score = self.suspicious_checker()
        # Calculate the final score
        ip_score = self.weights["ip"] * (ip_score)
        length_score = self.weights["length"] * (length_score)
        random_char_score = self.weights["random_char"] * (random_char_score)
        sus_score = self.weights["suspicious"] * (sus_score)

        criteria = [ip_score, length_score, random_char_score, sus_score]
        criteria = [x for x in criteria if x is not None]
        domain_score = sum(criteria) / len(criteria)
        return domain_score

    
    def prefix_checker(self, url):
        try:
            scheme = self.parsed.scheme.lower()
            if scheme == "http":
                return 0.25
            elif scheme == "https":
                if random.random() < 0.6:
                    return 1.0
                else:
                    return 0.75
            else:
                return 0.50  # fallback for unusual schemes like ftp or blank
        except AttributeError:
            return 0.25
            log = "Error: Invalid URL format"
            print(log)

    # Check the TLD of the URL   
    def TLD_checker(self):
        credible_tlds = ["com", "org", "net", "edu", "gov",
            "mil", "int", "ca", "de", "uk",
            "fr", "jp", "au", "us", "ch",
            "nl", "in", "it", "es", "se"]
        
        less_credible_tlds = ["info", "biz", "co", "me", "tv",
            "cc", "xyz", "site", "online", "store",
            "app", "io", "cloud", "pro", "tech"]
        
        sus_tlds  = [
            "tk", "ml", "ga", "cf", "gq",
            "top", "work", "support", "click", "review",
            "zip", "cam", "party", "loan", "download",
            "stream", "men", "host", "faith", "accountants"
            ]
        
        domain = self.parsed.netloc
        if not domain:
            return 0.25
        tld = domain.split(".")[-1]
        if not tld:
            return 0.25
        # check for common TLDs
        if tld in credible_tlds:
            return 1.0
        
        # check for less common TLDs
        elif tld in less_credible_tlds:
            return 0.75
        
        # check for sketchy TLDs
        elif tld in sus_tlds:
            return 0.25
        else:
            return 0.50
          
    # Check if the URL contains an IP address    
    def IP_address_checker(self):
        domain = self.parsed.netloc
        if domain.replace(".", "").isdigit():
            return 0.25
        else:
            return 1.0
    # Check the length of the URL path
    # If the length is greater than 20, it is suspicious
    def length_checker(self):
        length = len(self.parsed.path)
        if length > 20:
            if length > 50:
                return 0.25
            else:
                return 0.44
        else:
            return 1.0

    # Check for random characters in the URL
    def random_char_checker(self):
        flaws = []
        if "@" in self.parsed.netloc:
            flaws.append(1)
        if "-" in self.parsed.netloc:
            if self.parsed.netloc.count("-") > 2:
                flaws.append(1)
        if "_" in self.parsed.netloc:
            if self.parsed.netloc.count("_") > 2:
                flaws.append(2)
        if len(flaws) > 0:
            return 0.25
        else:
            return 1.0
            
    def suspicious_checker(self):
        # Check for suspicious patterns in the URL    
        for pattern in sus_patterns:
            if pattern in self.parsed.path.lower():
                return 0.25
        return 1.0 # If no suspicious patterns found, return a higher score
    
    def trust_ranking(self, score):
        # Determine the trust ranking based 
        if score:
            if score > 3:
                return "Very Authentic."
            elif score > 2:
                return "A bit sketchy. But still reliable."
            elif score > 1:
                return "Very sketchy. Not Authentic."
            else:
                return "Seems Phishy. Not Authentic."
        else:
            return "Error: Invalid score"
            log = "Error: Invalid score"
            print(log)

        
# the function that validates a url    

# Check if the URL is empty or invalid
if not UrlLink:
    print("URL is empty")
elif not validators.url(UrlLink):
    print("Invalid URL format")
else:
    checker = PhishingDetector(UrlLink)
    result = checker.parse_url()
    Phish = checker.scrutinize()
    checker.identify_tests(checker.p_score, checker.t_score, checker.d_score, checker.w_score)
    print("Phishing Status:", Phish[0])
    print("Phishing Status:", Phish[1])
    print("Phishing Status:", Phish[2])
    
    print(result)
    # print(f"BlackList: {BlackList}")
