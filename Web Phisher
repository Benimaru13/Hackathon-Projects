import random, math, time
from urllib.parse import urlparse
import openai, os, validators, whois
from dotenv import load_dotenv

load_dotenv
# Phishing Detector
# Idea s...ingenuity, prototypes
# SCALE: 1-5
# if less than pass: NOT RELIABLE AT ALLL 
# if barely pass: then less reliable 
# if pass well: then alr, reliable
AUTHENTIC_WEBSITES_LIST = [
    "google.com", "youtube.com", "facebook.com", "instagram.com", "chatgpt.com", "x.com",
      "whatsapp.com", "wikipedia.org", "reddit.com", "yahoo.co.jp", "yahoo.com", "yandex.ru",
      "tiktok.com", "amazon.com", "baidu.com", "bet.br", "microsoftonline.com", "linkedin.com",
      "netflix.com", "naver.com", "live.com", "dzen.ru", "office.com", "bing.com", "temu.com",
      "pinterest.com", "bilibili.com", "microsoft.com", "twitch.tv", "vk.com", "mail.ru", 
      "news.yahoo.co.jp", "sharepoint.com", "fandom.com", "globo.com", "canva.com", "weather.com",
      "samsung.com", "t.me", "duckduckgo.com", "nytimes.com", "zoom.us", "spotify.com", "discord.com",
      "apple.com", "imdb.com"

]

BlackList = []
# global variables
# Boolean Variables


UrlLink = "skibidi link"
CriteriaCount = 0
# Helper Functions
def get_base_domain(netloc):
    domain = netloc.split(':')[0]
    parts = domain.split('.')
    if len(parts) > 2:
        domain = '.'.join(parts[-2:])
    return domain


def get_whois_info(url):
    try:
        domain = urlparse(url).netloc
        whois_info = whois.whois(domain)
        age = whois_info.creation_date
        if age:
            age = age[0] if isinstance(age, list) else age
            if isinstance(age, str):
                age = time.strptime(age, "%Y-%m-%d %H:%M:%S")
            elif isinstance(age, int):
                age = time.localtime(age)
            else:
                return 1
            current_time = time.localtime()
            age_in_days = (time.mktime(current_time) - time.mktime(age)) / (24 * 3600)
            if age_in_days < 30:
                return 1
            elif age_in_days < 365:
                return 2
            else:
                return 3
        else:
            return 1
    except Exception as e:
        print(f"Error fetching WHOIS info: {e}")
        return 1


# PhishingDetector Class
class PhishingDetector:
    def __init__(self, URL):
        self.url = URL
        self.parsed = urlparse(self.url)
        self.is_safe = True

    # parses the URL and extracts components
    # such as scheme, domain, path, and query parameters
    def parse_url(self):
        print("Scheme (http or https):", self.parsed.scheme)
        print("Domain:", self.parsed.netloc)
        print("Path:", self.parsed.path)
        print("Query Params:", self.parsed.query)
        return self.parsed

    # Scrutinizes the URL to determine if it is phishing
    # or not based on various checks 
    def scrutinize(self):
        base_domain = get_base_domain(self.parsed.netloc)
        if base_domain in AUTHENTIC_WEBSITES_LIST:
            # Check if the URL is in the list of authentic websites
            self.is_safe = True
            final_score = 4.0
            log = "The URL is safe."
        elif base_domain in BlackList:
            # Check if the URL is in the blacklist
            self.is_safe = False
            final_score = 0.0
            log = "The URL is phishing and has already been blacklisted."
        else:
            prefix_score = self.prefix_checker(self.url)
            tld_score = self.TLD_checker()
            domain_score = self.domain_heuristics()
            whois_age_score = get_whois_info(self.url)
            # Calculate the final score
            final_score = (prefix_score + tld_score + domain_score + whois_age_score) / 4
            # Determine if the URL is phishing based on the final score
            final_score = round(final_score, 2)
            if final_score < 2:
                self.is_safe = False
                log = "The URL is phishing."
                BlackList.append(self.parsed.netloc)
                log += (" The URL has been added to the blacklist.")
                # print(log)
            else:
                self.is_safe = True
                log = "The URL is safe."
                # print(log) 
            trust_review = self.trust_ranking(final_score)
        return trust_review, str(final_score) + " out of 4.0", log      
        
    def domain_heuristics(self):
        prefix_score = self.prefix_checker(self.url)
        tld_score = self.TLD_checker()
        ip_score = self.IP_address_checker()
        length_score = self.length_checker()
        random_char_score = self.random_char_checker()
        sus_score = self.suspicious_checker()
        # Calculate the final score
        criteria = [prefix_score, tld_score, ip_score, length_score, random_char_score, sus_score]
        criteria = [x for x in criteria if x is not None]
        domain_score = sum(criteria) / len(criteria)
        return domain_score


    def prefix_checker(self, url):
        try:
            scheme = self.parsed.scheme.lower()
            if scheme == "http":
                return 1
            elif scheme == "https":
                if random.random() < 0.6:
                    return 1.5
                else:
                    return 2
            else:
                return 1  # fallback for unusual schemes like ftp or blank
        except AttributeError:
            return 1
            log = "Error: Invalid URL format"
            print(log)

    # Check the TLD of the URL   
    def TLD_checker(self):
        domain = self.parsed.netloc
        if not domain:
            return 1
        tld = domain.split(".")[-1]
        if not tld:
            return 1
        # check for common TLDs
        if tld in ["com", "net", "org", "io", "co"]:
            return 2.5
        # check for less common TLDs
        elif tld in ["info", 'biz', 'xyz', 'top']:
            return 1.5
        # check for sketchy TLDs
        elif tld in ["tk", "ga", "cf", "gq", "ml"]:
            return 1
        else:
            return 1
        
    
    # Check if the URL contains an IP address    
    def IP_address_checker(self):
        domain = self.parsed.netloc
        if domain.replace(".", "").isdigit():
            return 1

    # Check the length of the URL path
    # If the length is greater than 20, it is suspicious
    def length_checker(self):
        length = len(self.parsed.path)
        if length > 20:
            if length > 50:
                return 1
            else:
                return 2

    # Check for random characters in the URL
    def random_char_checker(self):
        flaws = []
        if "@" in self.parsed.netloc:
            flaws.append(1)
        if "-" in self.parsed.netloc:
            if self.parsed.netloc.count("-") > 2:
                flaws.append(1)
        if "_" in self.parsed.netloc:
            if self.parsed.netloc.count("_") > 2:
                flaws.append(1)
        if len(flaws) > 0:
            return 1
            
    def suspicious_checker(self):
        # Check for suspicious patterns in the URL
        sus_patterns = ["login", "secure", "update", "verify", "account", "confirm"]
        for pattern in sus_patterns:
            if pattern in self.parsed.path.lower():
                return 1
    
    def trust_ranking(self, score):
        # Determine the trust ranking based on the score
        if score > 4.5:
            return "Very Authentic."
        elif score > 3.5:
            return "A bit sketchy. But still reliable."
        elif score > 2.5:
            return "Very sketchy. Not Authentic."
        else:
            return "Seems Phishy. Not Authentic."

        
# the function that validates a url       
if not validators.url(UrlLink):
    print("Invalid URL format")
else:
    checker = PhishingDetector(UrlLink)
    result = checker.parse_url()
    Phish = checker.scrutinize()
    print("Phishing Status:", Phish[0])
    print("Phishing Status:", Phish[1])
    print("Phishing Status:", Phish[2])
    print(result)
    print(f"BlackList: {BlackList}")
